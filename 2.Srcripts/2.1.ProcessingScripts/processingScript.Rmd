---
title: "Scripts for Higiene Project"
author: "Flores-Kanter PE"
output:
  pdf_document:
    toc: yes
  html_document:
    code_folding: show
    theme: paper
    highlight: pygments
    toc: yes
---

```{r}
if(!"devtools" %in% row.names(installed.packages())){
  install.packages("devtools")
}
devtools::install_github("AlexChristensen/SemNeT")

library(stringi)
library(stringr)
library(qdap)
library(NLP)
library(tm)
library(SemNeT)
library(NetworkToolbox)
library(tidyverse)
```


#Data Processing Script

```{r}

text.df<-read.csv("txt_viejos.csv", sep = ";", header = TRUE) #Cambiar bases de acuerdo a los objetivos. "txt_viejos.csv"
grp<-read.csv("group_viejos.csv", sep = ";", header = TRUE) #Cambiar bases de acuerdo a los objetivos. "group_viejos.csv"

#The first function is a wrapper for the base R tolower function.

# Return NA instead of tolower error
tryTolower <- function(x){
# return NA when there is an error
y = NA
# tryCatch error
try_error = tryCatch(tolower(x), error = function(e) e)
# if not an error
if (!inherits(try_error, 'error'))
y = tolower(x)
return(y)
}

#Next you will define our stopwords.
custom.stopwords <- stopwords('spanish')


#Next you will include the new tryTolower function as part of a larger preprocessing function. Here you create a function called clean.corpus.
clean.corpus<-function(corpus){
corpus <- tm_map(corpus,
content_transformer(tryTolower))
corpus <- tm_map(corpus, removeWords,
custom.stopwords)
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, removeNumbers)
return(corpus)
}

#Before applying these cleaning functions, you need to define the tweets object as your corpus or collection of natural language documents. Additionally you are preserving the metadata about each document.
tweets<-data.frame(doc_id=seq(1:nrow(text.df)),text=text.df$Texto)

corpus <- VCorpus(DataframeSource(tweets))

#With all the preprocessing transformation functions organized you must now apply them to the DeltaAssist corpus.
corpus<-clean.corpus(corpus)

#Frequent Terms and Associations
#First, you create a new object called tdm, which is a list object used by the tm package.
tdm<-TermDocumentMatrix(corpus,control=list(weighting=weightTf))
tdm.tweets.m<-as.matrix(tdm)



## EGAnet (Golino)
corpus.stem <- tm_map(corpus, stemDocument)
dtm <- DocumentTermMatrix(corpus.stem)
dtm.sparsity.rm <- removeSparseTerms(dtm, 0.90)

dtm.data <- as.data.frame(as.matrix(dtm.sparsity.rm))
head(dtm.data)
colnames(dtm.data)


##SemNet (Christensen)

Abinar <- binarize(dtm.data)

## Change column names (variable names)
colnames(grp) <- c("conditions","times")


# Attach 'Group' variable to the binary response matrix
behav <- cbind(grp, Abinar)

#write.csv(behav, "binaryResponse_jovenes.csv", row.names = FALSE)
write.csv(behav, "binaryResponse_viejos.csv", row.names = FALSE)

# Create groups response matrices

sinHigiene_e1 <-behav %>% 
  filter(conditions=="sinhigiene" & times=="E1") %>% 
select(-c("conditions","times"))

sinHigiene_e2 <-behav %>% 
  filter(conditions=="sinhigiene" & times=="E2") %>% 
select(-c("conditions","times"))

sinHigiene_t1 <-behav %>% 
  filter(conditions=="sinhigiene" & times=="T1") %>% 
select(-c("conditions","times"))

sinHigiene_t2 <-behav %>% 
  filter(conditions=="sinhigiene" & times=="T2") %>% 
select(-c("conditions","times"))

conHigiene_e1 <-behav %>% 
  filter(conditions=="conhigiene" & times=="E1") %>% 
select(-c("conditions","times"))

conHigiene_e2 <-behav %>% 
  filter(conditions=="conhigiene" & times=="E2") %>% 
select(-c("conditions","times"))

conHigiene_t1 <-behav %>% 
  filter(conditions=="conhigiene" & times=="T1") %>% 
select(-c("conditions","times"))

conHigiene_t2 <-behav %>% 
  filter(conditions=="conhigiene" & times=="T2") %>% 
select(-c("conditions","times"))


```



